{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NULabTMN/homework-3-gmac98/blob/main/classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fniEAr7Kdsb"
      },
      "source": [
        "# Text Classification\n",
        "\n",
        "In class, we spent some time on text classification, including naive Bayes classifiers.  We focused on these models not only because they are simple to implement and fairly effective, but also because of their similarity to widely used bag-of-words retrieval models such as BM25 and query likelihood.\n",
        "\n",
        "Your task is to write a naive Bayes text categorization system to predict whether movie reviews are positive or negative.  The data for this **sentiment analysis** task were first assembled and published in Bo Pang and Lillian Lee, &ldquo;A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts&rdquo;, _Proceedings of the Association for Computational Linguistics_, 2004.\n",
        "\n",
        "## Loading the data\n",
        "\n",
        "First we load the training, development, and test splits of this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qou7WSViJ7j2"
      },
      "source": [
        "import json\n",
        "from urllib.request import urlopen\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5b269H_ImDz"
      },
      "source": [
        "# Read one JSON record per line\n",
        "def read_jsonl(f):\n",
        "  res = []\n",
        "  for line in f:\n",
        "    res.append(json.loads(line))\n",
        "  return res"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly9sQzxqKNsr"
      },
      "source": [
        "If you're working offline, you could modify this code to read from the copies of the data in the repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3DDh5ytJrTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f87ac71-2ed5-4f3b-f51e-e47858b336e3"
      },
      "source": [
        "train = read_jsonl(urlopen(\"https://github.com/dasmiq/cs6200-hw3/blob/main/train.json?raw=true\"))\n",
        "dev = read_jsonl(urlopen(\"https://github.com/dasmiq/cs6200-hw3/blob/main/dev.json?raw=true\"))\n",
        "test = read_jsonl(urlopen(\"https://github.com/dasmiq/cs6200-hw3/blob/main/test.json?raw=true\"))\n",
        "\n",
        "print(len(train))\n",
        "print(len(dev))\n",
        "print(len(test))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1600\n",
            "200\n",
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbvCSANLNSAt"
      },
      "source": [
        "Each of these subsets of the data is a list of documents, and each document has a unique identifier (`id`) and text (`text`). The training and development documents, in addition, have been labeled with a `class`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV_4DmkNKaz5",
        "outputId": "752b4619-bb6b-4665-a979-6344e11da521"
      },
      "source": [
        "print(train[0])\n",
        "print(dev[0])\n",
        "print(test[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '12178', 'class': 'neg', 'text': \"the sequel to the fugitive ( 1993 ) , u . s marshals is an average thriller using it's association with the fugitive just so it can make a few extra bucks . \\ntommy lee jones returns to his role as chief deputy samuel gerard , the grizzly cop who was after harrison ford in the fugitive . \\nthis time , he's after fugitive mark sheridan ( snipes ) who the police think killed two fbi agents , but of course he's been set up , and when the police plane escort he ( and gerard ) are riding crashes , he makes a run for it , gerard not so hot on his tail . \\nwhat follows is about 2 hours of action , brought to us by the director of executive decision ( 1995 ) , another film curiously involving a plane . \\nwhen comparing this movie to the fugitive , the prequel is far superior . \\nbut even on it's own , u . s marshals is a pretty lousy movie . \\nwhile the original was reasonably intelligent , and had a fugitive to root for , the audience feels strangely distanced from snipes fugitive , mainly because we know so little about him until way into the film's overlong running time . \\nwhile the fugitive gave a little time to develop harrison fords character , u . s marshals is straight in there , pulling it's trump card ( the place crash ) almost immediately . \\nto be honest , i couldn't care less if snipe's character got captured or not . \\nsnipes performance is average , and his character gets surprisingly little screen time ( considering he is the fugitive ) for reasons i'll explain later . \\ntommy lee jones is as fine as ever , although his role is hardly a challenge . \\nthe sense of deja vu was overwhelming . \\ni know it's the same role from the fugitive , but there is seemingly no attempt to develop his character from the last film . \\nthere's a few 'nudge nudge' references from the first film , but apart from that , nothing . \\nnobody even mentions harrison ford in this movie . \\ndowney jnr is ok as fbi agent john royce who's roped into the chase , and sexy french actress irene jacob wanders in and out of the movie as snipes wife . \\nnobody looks as if their having a good time , and all the performers are on autopilot . \\nthe supporting cast are o . k , but when it comes to performances , u . s marshals falls flat on it's face . \\nit doesn't fare any better plot-wise , either . \\nthe film starts off with gerard trying to capture snipes , but then veers off into terrorists territory . \\nas said above , the film is an normal , average terrorist thriller , seen a million times before , but is using the fugitive reputation to make it sound more professional . \\nif this film wasn't the 'sequel' to the fugitive , this would probably go straight to video , or not even been made at all , because it's so average . \\nand because of the film's plot changing it's course , snipes is almost forgotten apart , and hardly features in the movie . \\nstuart baird , the director , is all right , but there's no particular scene that stands out . \\nit's director is the usual action movie style . \\neven the plane crash is sorely lacking in tension . \\nthe special effects are nice , especially the plane crash , and there's a good ( if very loud ) soundtrack by veteran composer jerry goldsmith . \\nbut in the end , u . s marshals suffers from an overwhelmingly lack of excitement . \\nsure , it's loud and dumb , but in the end u . s marshals just ain't fun . \\nbored performers and a lackluster plot and script , do not make a good action movie . \\noverall rating= review by david wilcock web space provided by geocities \\n\"}\n",
            "{'id': '20531', 'class': 'neg', 'text': 'under any other circumstances , i would not be discussing the ending of a film to the extent that i will in this particular review . \\nhowever , in order to fully explain exactly how and why this movie is so awful , a minute dissection of the ending is necessary . \\neven though i will not reveal the details of the last scenes , do proceed at your own risk . \\nthe movie opens ( quite poorly , i might add ) as child psychologist malcolm crowe ( bruce willis , looking like he was dragged out of his trailer at the wee hours of the morning to shoot each scene ) and his wife are intruded upon by one of malcolm\\'s past patients . \\ndistraught , the suicidal man ( a cameo by new kid on the block donnie wahlberg ) shoots malcolm and then turns the gun on himself . \\ncut to the \" next fall \" , as we find the good doctor quietly observing his latest case , a trouble young man named cole ( haley joel osment , one of the only child actors in a while i didn\\'t want to bludgeoned over the head with a blunt instrument ) . \\nafter about 45 minutes of seemingly unrelated freak occurrences , we learn that cole has \" the sixth sense \" , the gift of being able to communicate with the dead . \\nand this , as they say , is where the healing begins . \\nthe sixth sense and its unexpected popularity is founded upon a twist ending that i knew going into the film ( one of roger ebert\\'s colleges was kind enough to give it away on a recent segment of \" siskel and ebert \" ) . \\nalthough i was at first enraged that an established film critic could so callously ruin a film for thousands of patrons , i soon realized that this turn of events could in fact have been a blessing in disguise ; i\\'ve always been a sucker for surprise endings ( my favorite movie is the usual suspects ) and rarely dislike a film that sports one . \\nhere , since i knew the major plot twist that was coming at the film\\'s conclusion , the possibly of being bamboozled into loving a bad movie solely because of its ending ( something i\\'ve fallen victim to in the past ) was eliminated . \\nand indeed , my viewing of the sixth sense did prove to be quite an enlightening experiment . \\nstripped of the element of surprise , the film was put to the task of showing what it really had , instead of simply hiding behind a shocking conclusion . \\nafter seeing its true colors , i came to the conclusion that the sixth sense is , despite what the many champions of the movie may say , void of any real power . \\nit\\'s a neat concept , but not one that justifies being made into a feature-length movie . \\nin fact , the sixth sense relies so strongly on its finale that the rest of the film develops as a sort of prelude to the supposedly earth-shattering revelation that is yet to come . \\nand when the final moments do come , it\\'s a huge letdown ; the end makes no sense at all . \\nit stupefied me with the heights of its ineptitude and is completely idiotic on a fundamental and very rare level . \\ni won\\'t go into any details , but suffice to say that , as far as i can tell , it negates to rest of the movie to such an extent that anyone who buys it even for a second must be suffering from a very acute case of attention deficit disorder . \\nnow , in all fairness , i cannot say for sure that i would have guessed the ending ( however stupid it may be ) had it not been revealed to me before hand . \\nhowever , i feel very confident that i , as well as anyone who had seen a few \" twilight zone \" episodes , would have seen it coming a mile away . \\nthe fact that movie-goers nation wide are surprised by the ending still has me stumped . \\nironically , to fully appreciate the best scene ( that of cole and malcolm attending a little girl\\'s funeral ) , the viewer is required to be aware of a very rare psychological disorder called munchausen syndrome by proxy . \\ni wouldn\\'t have even known about this mental disease if i hadn\\'t , by pure dumb luck , caught \" dateline nbc \" the other week when they did a feature story on it . \\ndespite being blessed with some really amazing cinematography and a brauva performance from osment ( where was this kid when casting calls were going out for the phantom menace ? ) , in the end , the sixth sense is too chalk-full of contradictions and just isn\\'t plausible enough to warrant even a slight recommendation . \\n'}\n",
            "{'id': '11471', 'text': 'the lives of older people in the twilight of their years attempting to come to grips with their shared histories and possible futures is a fascinating topic . \\nfinding an all-star cast for such a film is a stroke of genius . \\ncombining all that with a three-time oscar-winning director ( robert benton of \" kramer vs . \\nkramer \" ) and creating a decidedly mediocre movie is the stuff of disappointment . \\nin yet another noir mystery set in hollywood -- how many of these have we seen during the past few years ? \\n-- the atmosphere is moody , the actors enjoyable to watch and the story goes nowhere . \\nover-70 harry ross ( paul newman ) is a washed up cop-turned-private eye-turned man friday trying to figure out how to live what remains of his life . \\nhe\\'s screwed up things pretty well ( \" i had a wife and daughter . \\nnow , i\\'m a drunk ) and is at a crossroads . \\na couple of years ago , he traveled to mexico to bring back mel ( reese witherspoon ) , the under-age daughter of jack ( gene hackman ) and catherine ( susan sarandon ) ames and now lives with them . \\nthe ames are former movie stars , past their prime and the three have become fast friends . \\none gets the impression that ross is just hanging out waiting for something to wake him up . \\nto fill his time , he does odd jobs for jack and falls in love with catherine . \\njack is in even worse shape than harry . \\nhe\\'s dying of cancer with only a year to live . \\nthings do turn more exciting when jack asks harry to drop off a sealed manila envelope for him . \\ninstead of the routine errand that ross expects , he walks into a barrage of bullets from the gun of another ex-cop who is , himself , full of bloody holes . \\nthis unsettling event gives the former detective a project to throw himself into and launches an investigation that revolves around the mysterious disappearance of catherine\\'s first husband 20 years before . \\nthrough a series of very complex and convoluted plot devices that involve murder ; blackmail ; guns ; mel\\'s mexico traveling partner and his parole officer ; ross\\'s former cop buddies , ex-lover and would-be sidekick , the tale finally ends up exactly where everyone expects it to . \\nit\\'s a film noir tradition that the story twists and turns down side roads for an unexpected finale , but here the journey meanders towards an ending that no one cares about . \\nthe only surprises are exactly whose face fits which role in the scenario . \\nby the time they show you , it doesn\\'t matter . \\nthe storyline gets goofier and goofier exemplified in ross\\'s relationship with rubin ( giancarlo esposito ) , a partner wannabe . \\nthese scenes are obviously designed to be comic relief , however they are neither . \\nrubin and ross have some past relationship but either it\\'s not explained or i didn\\'t care enough at that point to remember . \\na running joke about where harry was supposedly shot while in mexico is probably meant to mirror his questions about whether he is still able to perform . \\nit\\'s also not funny , doesn\\'t connect and keeps on showing up long after it has run its course . \\non the positive side , it\\'s often enjoyable to watch the seasoned actors on the screen . \\nthe three leads all have well-deserved academy awards and turn in accomplished , if not extraordinary jobs . \\nnewman is a grand actor , but doesn\\'t seem quite suited to the dark film style . \\nhe is a bit too clean and understated to come across as desperate and down and out . \\nhackman , also low-key , is believable but lacks sparkle . \\nsarandon comes across well as an sultry older babe although she is one-dimensional . \\nthe actors do what they can with lame dialog , but they can\\'t pull the film out of the hole it\\'s dug for itself . \\njames garner who plays ross\\'s old buddy ex-cop raymond hope is always a treat , but even he half-heartedly struggles through lines like \" i\\'m glad they didn\\'t shoot your pecker off . \" \\nthe best part of the film is the look at old friends , how their relationships change over the years and the difficult choices they must make . \\nthe genuinely easy and casual interactions among the actors hint that being on the set was much more interesting than what ended up on the screen \\nthe film doesn\\'t run very long before the audience realizes that it\\'s hopeless . \\nthe only reason for watching is the actors . \\nit reminds me of disaster movies such as \" towering inferno \" where the star power is supposed to make everyone ignore the film\\'s problems . \\nin a better world , there would have been second-rate actors in this second-rate movie and the ones here would have been saved for something better . \\nof course , we don\\'t live in that better world , but you could make yours a little nicer by choosing a different movie . \\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz6U09YLOPZa"
      },
      "source": [
        "## Collecting term statistics\n",
        "\n",
        "The text has been pre-tokenized and lower-cased.  All you have to do to get the individual terms in a review is to split the tokens by whitespace, a sequence of spaces and/or newlines."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## To diplay all output in window screen\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "g-JuJq8Z5JMm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tuples_from_file(training_file_path):\n",
        "  \"\"\"\n",
        "  Generates tuples from list of dictionary\n",
        "  Return:\n",
        "    a list of tuples of strings formatted [(id, label, example_text), (id, label, example_text)....]\n",
        "  \"\"\"\n",
        "  list1 = []\n",
        "  for dic in training_file_path:\n",
        "    l1 = []\n",
        "    for key, val in dic.items():\n",
        "        l1.append(val)\n",
        "    v1 = tuple(l1)\n",
        "    list1.append(v1)\n",
        "  return list1\n",
        "\n",
        "listOfTrainingExamples = generate_tuples_from_file(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lBzB_zA6zKTF",
        "outputId": "17e94538-367d-488a-9785-4020ddfc065d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyBgOVvzO7pr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "3c590d90-b808-4454-a11a-ad0fffb99420"
      },
      "source": [
        "## TODO: Write a function to convert a document into a collection of terms and their counts.\n",
        "## Convert the lists of documents in the training, development, and test sets into these collections of terms and counts.\n",
        "def tokenize1(data):\n",
        "    return data.split()\n",
        "\n",
        "def term_statistics(examples, tokenize):\n",
        "    positive_lines = []\n",
        "    negative_lines = []\n",
        "    \n",
        "    for i in examples:\n",
        "        if i[1] == \"pos\":\n",
        "            positive_lines.append(i[2])\n",
        "        if i[1] == \"neg\":\n",
        "            negative_lines.append(i[2])\n",
        "            \n",
        "    N_positive = len(positive_lines)                   #Number of positive docs\n",
        "    N_negative = len(negative_lines)                   #Number of negative docs\n",
        "    \n",
        "    positive_vocabulary_list = []\n",
        "    for line in positive_lines:\n",
        "        words = tokenize(line)\n",
        "        temp_positive_vocabulary_list = []\n",
        "        for word in words:\n",
        "            if word!='':\n",
        "                temp_positive_vocabulary_list.append(word)  #all words in positive doc\n",
        "        tpvl = []\n",
        "        tpvl = set(temp_positive_vocabulary_list)\n",
        "        for word in tpvl:\n",
        "            positive_vocabulary_list.append(word)\n",
        "    \n",
        "    \n",
        "    negative_vocabulary_list = []\n",
        "    for line in negative_lines:\n",
        "        words = tokenize(line)\n",
        "        temp_negative_vocabulary_list = []\n",
        "        for word in words:\n",
        "            if word!='':\n",
        "                temp_negative_vocabulary_list.append(word)  #words in negative doc\n",
        "        tnvl = []\n",
        "        tnvl = set(temp_negative_vocabulary_list)\n",
        "        for word in tnvl:\n",
        "            negative_vocabulary_list.append(word)\n",
        "            \n",
        "            \n",
        "    vocabulary_list = []\n",
        "    vocabulary_list = positive_vocabulary_list + negative_vocabulary_list   #all words from all docs (including duplicates)\n",
        "    vocabulary = set(vocabulary_list)                        #unique words from pos and neg doc\n",
        "\n",
        "    return N_positive, N_negative, vocabulary, positive_vocabulary_list, negative_vocabulary_list\n",
        "\n",
        "N_positive, N_negative, vocabulary, positive_vocabulary_list, negative_vocabulary_list = term_statistics(listOfTrainingExamples, tokenize1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lab8VS4lVf_m"
      },
      "source": [
        "The statistics for individual documents will be useful in predicting the class of those documents, e.g., in the test set.\n",
        "\n",
        "Now, you will collect the statistics used to estimate the parameters of a naive Bayes model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9H67SPHV2sw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b307bafd-87c9-4dd7-d52e-eac084510558"
      },
      "source": [
        "## TODO: Write a function to take a list of document statistics and produce a dictionary of term counts in each class.\n",
        "## Your output will look something like this:\n",
        "\n",
        "# fakeData = {'pos': {'the': 10000, 'and': 800},\n",
        "#             'neg': {'the': 1001, 'and': 799}}\n",
        "\n",
        "def termCountDict(positive_vocabulary_list, negative_vocabulary_list):\n",
        "    positive_word_count = Counter(positive_vocabulary_list)  \n",
        "    negative_word_count = Counter(negative_vocabulary_list)\n",
        "    sentimentData = {'pos': positive_word_count, 'neg': negative_word_count}\n",
        "\n",
        "    return sentimentData\n",
        "    \n",
        "sentimentData = termCountDict(positive_vocabulary_list, negative_vocabulary_list)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTQgq1wcWuC5"
      },
      "source": [
        "## Estimating naive Bayes parameters\n",
        "\n",
        "As we discussed in class, you could use simple maximum-likelihood estimation for naive Bayes parameters, i.e., computing the relative frequency of a term given a class. The problem is that the relative frequency of words not seen in the training data will be zero, e.g., $p(\\texttt{aardvark} | \\texttt{pos}) = \\frac{0}{\\textrm{tokens in the positive training data}}$.\n",
        "\n",
        "To avoid this problem, estimate the parameters with **add-1 (Laplace) smoothing**. In other words, add an additional count of 1 to each word type. Then, to make the probability distribution sum to 1, add a count of 1 for each vocabulary word to the denominator. For our `aardvark` example, we would now have, for vocabulary $V$, $p(\\texttt{aardvark} | \\texttt{pos}) = \\frac{0 + 1}{N_{\\texttt{pos}} + 1 \\cdot |V|}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iw7JL9SWlOk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "02cb5402-bc4b-47f5-e76a-f37250b752bc"
      },
      "source": [
        "## TODO: Write a function to compute the add-1 smoothed parameters for a naive Bayes model given the term statistics you computed above.\n",
        "## Collect these parameters for the training set.\n",
        "def naive(vocabulary, sentimentData, alpha):\n",
        "    positive_likelihood = {}\n",
        "    negative_likelihood = {}\n",
        "\n",
        "    for i in vocabulary:\n",
        "        positive_likelihood[i] = math.log( (sentimentData['pos'][i] + alpha) / (len(positive_vocabulary_list) + alpha * len(vocabulary)) )\n",
        "        negative_likelihood[i] = math.log( (sentimentData['neg'][i] + alpha) / (len(negative_vocabulary_list) + alpha * len(vocabulary)) )\n",
        "\n",
        "    return positive_likelihood, negative_likelihood\n",
        "\n",
        "positive_likelihood, negative_likelihood = naive(vocabulary, sentimentData, 1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdpWyzIPZdgD"
      },
      "source": [
        "What terms are likely to be important for prediction?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSzF8M-kY-VZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "7e91c78b-691c-45c9-a244-f631844f87f0"
      },
      "source": [
        "## TODO: Print a list of the 25 terms with the highest log ratio of positive to negative weight.\n",
        "p2n_ratio = {}\n",
        "for i in vocabulary:\n",
        "    p2n_ratio[i] = positive_likelihood[i] / negative_likelihood[i]\n",
        "\n",
        "print('25 terms with the highest log ratio of positive to negative weight', sorted(p2n_ratio.items(), key=lambda x: x[1], reverse=True)[:25])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25 terms with the highest log ratio of positive to negative weight [('atrocious', 1.302180758505219), ('jolie', 1.2695013814236602), ('3000', 1.2695013814236602), ('seagal', 1.2695013814236602), ('ludicrous', 1.2641525109012752), ('dud', 1.2601756887390128), ('incoherent', 1.2601756887390128), ('justin', 1.2601756887390128), ('angelina', 1.2601756887390128), ('insulting', 1.250933980117748), ('wielding', 1.2502558715972305), ('shoddy', 1.239648073389705), ('degenerates', 1.239648073389705), ('overwrought', 1.239648073389705), ('chuckle', 1.2380386728556514), ('uninvolving', 1.2380386728556514), ('idiotic', 1.2310745815650717), ('nutty', 1.2282332175081239), ('silverstone', 1.2282332175081239), ('bruckheimer', 1.2282332175081239), ('horrid', 1.2282332175081239), ('tedium', 1.2158568536544883), ('feeble', 1.2158568536544883), ('coyote', 1.2158568536544883), ('hodgepodge', 1.2158568536544883)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkX55joVZJGJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "284a54e4-44da-40ce-e62f-207540bb1dfd"
      },
      "source": [
        "## TODO: Print a list of the 25 terms with the highest log ration of negative to positive weight.\n",
        "n2p_ratio = {}\n",
        "for i in vocabulary:\n",
        "    n2p_ratio[i] = negative_likelihood[i] / positive_likelihood[i]\n",
        "\n",
        "print('25 terms with the highest log ratio of negative to positive weight', sorted(n2p_ratio.items(), key=lambda x: x[1], reverse=True)[:25])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25 terms with the highest log ratio of negative to positive weight [('outstanding', 1.2772588742848134), ('symbol', 1.2646964445150808), ('lovingly', 1.2560349871785232), (\"son's\", 1.2560349871785232), ('fashioned', 1.2560349871785232), ('thematic', 1.2468625664196493), ('dread', 1.2371049081709438), ('melancholy', 1.2371049081709438), ('religion', 1.2370176408155211), ('vulnerable', 1.2308066065007344), ('comforts', 1.2266695139817518), ('characteristic', 1.2266695139817518), ('en', 1.2266695139817518), ('masterfully', 1.2266695139817518), ('ideals', 1.2266695139817518), ('astounding', 1.2176046005546983), ('avoids', 1.2176046005546983), ('gattaca', 1.215439022783148), ('methodical', 1.215439022783148), ('exhilarating', 1.215439022783148), ('symbols', 1.215439022783148), ('steady', 1.215439022783148), ('tobey', 1.215439022783148), ('fascination', 1.2105602849041193), ('judges', 1.2032611988477546)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbx_izd7ZUuo"
      },
      "source": [
        "Now, given the parameters you've estimated, you can make predictions about new documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSh2STb4UjDm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6002acdb-42c1-4ade-da32-c83dc0c75965"
      },
      "source": [
        "## TODO: Compute the predictions of your model for each document in the development data.\n",
        "def classify(N_positive, N_negative, data, positive_likelihood, negative_likelihood, tokenize):\n",
        "    \"\"\"\n",
        "    Score a given piece of text\n",
        "    Return: dict of class: score mappings\n",
        "    \"\"\"\n",
        "    words = tokenize(data)\n",
        "    test_words = []\n",
        "    for i in words:\n",
        "        if i!='':\n",
        "            if i in positive_likelihood:\n",
        "              test_words.append(i)\n",
        "    \n",
        "    N_doc = N_positive + N_negative                    #Total number of docs\n",
        "\n",
        "    prior_positive = math.log((N_positive/N_doc))     #Logarithmic Prior\n",
        "    prior_negative = math.log((N_negative/N_doc))\n",
        "\n",
        "    positive_value = prior_positive\n",
        "    negative_value = prior_negative\n",
        "    for i in test_words:\n",
        "        positive_value = ( positive_value) + ( positive_likelihood.get(i))\n",
        "        negative_value = ( negative_value) + ( negative_likelihood.get(i))\n",
        "        \n",
        "    # positive_value = np.exp(positive_value)\n",
        "    # negative_value = np.exp(negative_value)\n",
        "    predScoreDistribution = {'pos': positive_value, 'neg': negative_value}\n",
        "    # print(predScoreDistribution)\n",
        "    if predScoreDistribution['pos'] >= predScoreDistribution['neg']:\n",
        "        return 'pos'\n",
        "    else:\n",
        "        return 'neg'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Write a function to compute the accuracy of predictions given correct classes.\n",
        "\n",
        "devClasses = [x['class'] for x in dev]\n",
        "\n",
        "listOfDevExamples = generate_tuples_from_file(dev)\n",
        "devPredictions1 = []\n",
        "for line in listOfDevExamples:\n",
        "      devPredictions1.append(classify(N_positive, N_negative, line[2], positive_likelihood, negative_likelihood, tokenize1))\n",
        "\n",
        "def accuracy(predicted_labels, gold_labels):\n",
        "    numerator = 0.0\n",
        "    denominator = len(predicted_labels)\n",
        "    for i in range(len(gold_labels)):\n",
        "        if gold_labels[i] == predicted_labels[i]:\n",
        "            numerator += 1\n",
        "    return numerator / denominator\n",
        "\n",
        "accuracy1 = accuracy(devPredictions1, devClasses)\n",
        "print(\"1st model, Accuracy:\", accuracy1)"
      ],
      "metadata": {
        "id": "UA2sdupHQ9Ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "48cdbb18-fda7-4c89-c9e6-fb12fef42cc3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1st model, Accuracy: 0.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgX4mD0uUvFR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "d7c15f39-1ada-4be1-c28c-cce163865b78"
      },
      "source": [
        "## TODO: Compute the predictions of this model on each document in the test set.\n",
        "listOfTestExamples = generate_tuples_from_file(test)\n",
        "testPredictions = []\n",
        "n = 0\n",
        "key_order = ['id', 'class', 'text']\n",
        "for line in listOfTestExamples:\n",
        "      testPredictions.append(classify(N_positive, N_negative, line[1], positive_likelihood, negative_likelihood, tokenize1))\n",
        "      test[n]['class'] = testPredictions[-1]\n",
        "      test[n] = {k : test[n][k] for k in key_order}\n",
        "      n += 1\n",
        "\n",
        "print(test[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '11471', 'class': 'neg', 'text': 'the lives of older people in the twilight of their years attempting to come to grips with their shared histories and possible futures is a fascinating topic . \\nfinding an all-star cast for such a film is a stroke of genius . \\ncombining all that with a three-time oscar-winning director ( robert benton of \" kramer vs . \\nkramer \" ) and creating a decidedly mediocre movie is the stuff of disappointment . \\nin yet another noir mystery set in hollywood -- how many of these have we seen during the past few years ? \\n-- the atmosphere is moody , the actors enjoyable to watch and the story goes nowhere . \\nover-70 harry ross ( paul newman ) is a washed up cop-turned-private eye-turned man friday trying to figure out how to live what remains of his life . \\nhe\\'s screwed up things pretty well ( \" i had a wife and daughter . \\nnow , i\\'m a drunk ) and is at a crossroads . \\na couple of years ago , he traveled to mexico to bring back mel ( reese witherspoon ) , the under-age daughter of jack ( gene hackman ) and catherine ( susan sarandon ) ames and now lives with them . \\nthe ames are former movie stars , past their prime and the three have become fast friends . \\none gets the impression that ross is just hanging out waiting for something to wake him up . \\nto fill his time , he does odd jobs for jack and falls in love with catherine . \\njack is in even worse shape than harry . \\nhe\\'s dying of cancer with only a year to live . \\nthings do turn more exciting when jack asks harry to drop off a sealed manila envelope for him . \\ninstead of the routine errand that ross expects , he walks into a barrage of bullets from the gun of another ex-cop who is , himself , full of bloody holes . \\nthis unsettling event gives the former detective a project to throw himself into and launches an investigation that revolves around the mysterious disappearance of catherine\\'s first husband 20 years before . \\nthrough a series of very complex and convoluted plot devices that involve murder ; blackmail ; guns ; mel\\'s mexico traveling partner and his parole officer ; ross\\'s former cop buddies , ex-lover and would-be sidekick , the tale finally ends up exactly where everyone expects it to . \\nit\\'s a film noir tradition that the story twists and turns down side roads for an unexpected finale , but here the journey meanders towards an ending that no one cares about . \\nthe only surprises are exactly whose face fits which role in the scenario . \\nby the time they show you , it doesn\\'t matter . \\nthe storyline gets goofier and goofier exemplified in ross\\'s relationship with rubin ( giancarlo esposito ) , a partner wannabe . \\nthese scenes are obviously designed to be comic relief , however they are neither . \\nrubin and ross have some past relationship but either it\\'s not explained or i didn\\'t care enough at that point to remember . \\na running joke about where harry was supposedly shot while in mexico is probably meant to mirror his questions about whether he is still able to perform . \\nit\\'s also not funny , doesn\\'t connect and keeps on showing up long after it has run its course . \\non the positive side , it\\'s often enjoyable to watch the seasoned actors on the screen . \\nthe three leads all have well-deserved academy awards and turn in accomplished , if not extraordinary jobs . \\nnewman is a grand actor , but doesn\\'t seem quite suited to the dark film style . \\nhe is a bit too clean and understated to come across as desperate and down and out . \\nhackman , also low-key , is believable but lacks sparkle . \\nsarandon comes across well as an sultry older babe although she is one-dimensional . \\nthe actors do what they can with lame dialog , but they can\\'t pull the film out of the hole it\\'s dug for itself . \\njames garner who plays ross\\'s old buddy ex-cop raymond hope is always a treat , but even he half-heartedly struggles through lines like \" i\\'m glad they didn\\'t shoot your pecker off . \" \\nthe best part of the film is the look at old friends , how their relationships change over the years and the difficult choices they must make . \\nthe genuinely easy and casual interactions among the actors hint that being on the set was much more interesting than what ended up on the screen \\nthe film doesn\\'t run very long before the audience realizes that it\\'s hopeless . \\nthe only reason for watching is the actors . \\nit reminds me of disaster movies such as \" towering inferno \" where the star power is supposed to make everyone ignore the film\\'s problems . \\nin a better world , there would have been second-rate actors in this second-rate movie and the ones here would have been saved for something better . \\nof course , we don\\'t live in that better world , but you could make yours a little nicer by choosing a different movie . \\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ExTuNkZiP7"
      },
      "source": [
        "## Comparing models with hypothesis testing (CS6200 only)\n",
        "\n",
        "Now that you've built a straightforward naive Bayes model, you can try to improve it. You could try stemming words, or including bigrams or trigrams, or adjusting the hyperparameter for Laplace smoothing, or trying a different smoothing method. What you try is up to you."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Try different features and/or smoothing.\n",
        "## Compute the predictions and the acuracy of your new model on development data.\n",
        "\n",
        "# Tokenizing and stemming\n",
        "def tokenize2(data):\n",
        "    data = data.split()\n",
        "    result = []\n",
        "    for i in range(len(data)):\n",
        "        word = data[i]\n",
        "        if word.endswith(\"sses\"):\n",
        "            result.append(word[:-2])\n",
        "        elif word.endswith(\"ing\"):\n",
        "            result.append(word[:-3])\n",
        "        elif word.endswith(\"ies\"):\n",
        "            result.append(word[:-2])\n",
        "        elif word.endswith(\"ss\"):\n",
        "            result.append(word[:-2])\n",
        "        elif word.endswith(\"ment\"):\n",
        "            result.append(word[:-4])\n",
        "        # elif word.endswith(\"s\"):\n",
        "        #     result.append(word[:-1])\n",
        "        else:\n",
        "            result.append(word)\n",
        "    return result\n",
        "\n",
        "\n",
        "N_positive2, N_negative2, vocabulary2, positive_vocabulary_list2, negative_vocabulary_list2 = term_statistics(listOfTrainingExamples, tokenize2)\n",
        "sentimentData2 = termCountDict(positive_vocabulary_list2, negative_vocabulary_list2)\n",
        "\n",
        "# Adjusting hyperparameter for Laplace smoothing to 4\n",
        "positive_likelihood2, negative_likelihood2 = naive(vocabulary2, sentimentData2, 4)\n",
        "\n",
        "devClasses = [x['class'] for x in dev]\n",
        "listOfDevExamples = generate_tuples_from_file(dev)\n",
        "devPredictions2 = []\n",
        "for line in listOfDevExamples:\n",
        "      devPredictions2.append(classify(N_positive2, N_negative2, line[2], positive_likelihood2, negative_likelihood2, tokenize2))\n",
        "\n",
        "accuracy2 = accuracy(devPredictions2, devClasses)\n",
        "print(\"2nd model, Accuracy:\", accuracy2)"
      ],
      "metadata": {
        "id": "Oe6Q118oNKHt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5a917e42-215e-4c2f-f737-8ad8c635b3a7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2nd model, Accuracy: 0.755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps your new model will be better than your original naive Bayes model above; perhaps it will be worse. (That won't affect your grade.) Your next step is to implement a permutation test to compare the two models. Refer to the material on the bootstrap and permutation tests at the end of the evaluation lecture."
      ],
      "metadata": {
        "id": "Si9C0EmmNYiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Compute the difference in accuracy between your first naive Bayes model and your new one.\n",
        "\n",
        "difference = np.abs(accuracy1 - accuracy2)\n",
        "print('Observed difference : ',difference)\n",
        "\n",
        "for i in range(0,len(devPredictions1)):\n",
        "    if devPredictions1[i] == 'pos':\n",
        "      devPredictions1[i] = 1\n",
        "    else:\n",
        "      devPredictions1[i] = 0\n",
        "\n",
        "for i in range(0,len(devPredictions2)):\n",
        "    if devPredictions2[i] == 'pos':\n",
        "      devPredictions2[i] = 1\n",
        "    else:\n",
        "      devPredictions2[i] = 0\n",
        "devPred1 = np.array(devPredictions1)\n",
        "devPred2 = np.array(devPredictions2)"
      ],
      "metadata": {
        "id": "w3HF-Xu-PDAh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0f186434-770a-44ab-ec57-95c34603e02e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observed difference :  0.03500000000000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Write a function that, given two vectors of predictions that have\n",
        "## the same length, swaps each pair of predictions with probability 0.5.\n",
        "def permute(y1, y2):\n",
        "  for i in range(len(y1)):\n",
        "    rand = random.random()\n",
        "    if rand > 0.5:\n",
        "      y1[i], y2[i] = y2[i], y1[i]\n",
        "\n",
        "  return y1, y2"
      ],
      "metadata": {
        "id": "0MFDrMHJTSDZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ff9d8263-6034-403f-be95-528fc27e5cda"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: If you run this permute function 500 times on the predictions of your two models,\n",
        "## it should produce 500 different pairs of vectors.\n",
        "## Compute the difference in accuracy for each of these 500 pairs, i.e., a vector of 500 differences.\n",
        "def testingDiff(xs, ys, nmc):\n",
        "    permutationDifferences = []\n",
        "    for j in range(nmc):\n",
        "        y1, y2 = permute(xs, ys)\n",
        "        diff = np.abs(np.mean(y1) - np.mean(y2))\n",
        "        permutationDifferences.append(diff)\n",
        "    return permutationDifferences\n",
        "\n",
        "permutationDifferences = testingDiff(devPred1, devPred2, 500)\n",
        "# print(permutationDifferences[:20])"
      ],
      "metadata": {
        "id": "5p77hnCJT3qN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "21f78b6c-2807-4080-cbc3-c485c7cb816d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Compute the proportion of these differences that are greater than the actual accuracy difference between your two models.\n",
        "## This proportion is the p-value computed by this permutation test.\n",
        "k = 0\n",
        "for i in range(len(permutationDifferences)):\n",
        "    if permutationDifferences[i] > difference:\n",
        "      k +=1\n",
        "\n",
        "print('p-value : ', k/500)"
      ],
      "metadata": {
        "id": "887iK4acxd9i",
        "outputId": "4aafa2ff-3de2-4eea-ed1e-ee4c2801289c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p-value :  0.012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## This code should plot a histogram of the differences between the permuted vectors\n",
        "## and the location of the observed accuracy difference.\n",
        "plt.hist(permutationDifferences, 20, label='Replicates', edgecolor='black')\n",
        "ylim = plt.ylim()\n",
        "plt.plot(2 * [difference], ylim, '--g', linewidth=3, label='Observed difference')\n",
        "plt.ylim(ylim)\n",
        "plt.legend()\n",
        "plt.xlabel('Difference in accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oBL94mc4Ve0P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "55d966d0-8bac-4806-e5cd-e2066d535b92"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEJCAYAAABv6GdPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAf6ElEQVR4nO3de3gV1f3v8ffXiISIN26eQEDQIpUoIAleikqUcpG7UsEbSrWmCip6KA8otoYWWz2t9ohaaKgUPeUnN4sC9XesgrSiok1SoOGiAoIkIFCwXASUwPr9McN2A7ntZG9mM3xezzMPa69Zs+a7J8mX2WtmrzHnHCIiEi6nBB2AiIjEn5K7iEgIKbmLiISQkruISAgpuYuIhJCSu4hICFWZ3M2suZm9Y2YrzWyFmY3w6/PMrNTMlvpLr6htHjGzNWb2sZn1SOQbEBGRY1lV97mbWTqQ7pwrMrMzgEJgADAI2OOc+81R7dsCrwCXAU2Bt4ELnXMHExC/iIiU49SqGjjnNgOb/fJuM1sFNKtkk/7AdOfc18BnZrYGL9F/UNEGjRo1ci1btowlbhFJQjv374yUz0o9K8BITg6FhYX/ds41Lm9dlck9mpm1BC4FPgQ6A/eb2R1AATDSOfclXuJfErVZCZX/Z0DLli0pKCiIJRQRSUI2ziJl97i+/Z5oZrahonXVvqBqZvWBV4GHnHO7gInABUAHvDP7p2MMKtfMCsysYNu2bbFsKiIiVahWcjezOniJfZpz7s8AzrktzrmDzrlDwGS8oReAUqB51OYZft0RnHP5zrls51x248blfqoQEZEaqs7dMga8CKxyzj0TVZ8e1ewGoNgvzwVuNrO6ZtYKaA18FL+QRUSkKtUZc+8MDAH+ZWZL/bpHgVvMrAPggPXAjwGccyvMbCawEigDhutOmRPHgQMHKCkpYf/+/UGHIkkoNTWVjIwM6tSpE3QoUoXq3C2zGLByVr1RyTZPAE/UIi4JSElJCWeccQYtW7bE+9Am4nHOsX37dkpKSmjVqlXQ4UgV9A1VOcL+/ftp2LChErscw8xo2LChPtWdIJTc5RhK7FIR/W6cOJTcJemUlJTQv39/WrduzQUXXMCIESP45ptvAJg6dSr3339/wBEeq379+lW2ycvL4ze/8b7Q/bOf/Yy3334bgHfffZfMzEw6dOjAvn37GDVqFJmZmYwaNSqhMUu4KblLUnHOceONNzJgwAA+/fRTPvnkE/bs2cPYsWMTts+ysrKE9V2Rn//853z/+98HYNq0aTzyyCMsXbqUevXqkZ+fz/Lly/n1r39drb6CiF+S3wmf3NMzWmBmNV7SM1oE/RYkysKFC0lNTeWHP/whACkpKfz2t79lypQp7N27F4CNGzeSk5ND69atGTduHABfffUVvXv3pn379lx88cXMmDEDgMLCQrp06UJWVhY9evRg8+bNAOTk5PDQQw+RnZ3NE088wXnnncehQ4cifTVv3pwDBw6wdu1aevbsSVZWFldffTWrV68G4LPPPuPKK6/kkksu4bHHHqvw/TzxxBNceOGFXHXVVXz88ceR+qFDhzJ79mz+8Ic/MHPmTH76059y22230a9fP/bs2UNWVhYzZsxg27ZtDBw4kE6dOtGpUyfee+89wPsUMGTIEDp37syQIUMqbXfXXXeRk5PD+eefz4QJEyIxvPzyy7Rr14727dszZMgQgAr7qa6O6R0jiwQrpukHktEXpRs5b/T8Gm+/4ak+cYxGamvFihVkZWUdUXfmmWfSokUL1qxZA8BHH31EcXExaWlpdOrUid69e7NhwwaaNm3KX/7yFwB27tzJgQMHeOCBB3j99ddp3LgxM2bMYOzYsUyZMgWAb775JjLtRVFREX/729+49tprmT9/Pj169KBOnTrk5uYyadIkWrduzYcffsiwYcNYuHAhI0aM4L777uOOO+7ghRdeKPe9FBYWMn36dJYuXUpZWRkdO3Y85r396Ec/YvHixfTp04cf/OAHgDfEs3Spd9fxrbfeysMPP8xVV13F559/To8ePVi1ahUAK1euZPHixdSrV6/SdqtXr+add95h9+7dtGnThvvuu49PPvmE8ePH8/7779OoUSN27NgBwIgRIyrspzoKcwur3VYS64RP7pJYeYvyGPe3cdVqe0/He8jvm39EXe68XCYXTY68frzL4+Tl5NUqpm7dutGwYUMAbrzxRhYvXkyvXr0YOXIko0ePpk+fPlx99dUUFxdTXFxMt27dADh48CDp6d9+927w4MFHlGfMmMG1117L9OnTGTZsGHv27OH999/npptuirT7+uuvAXjvvfd49dVXARgyZAijR48+Js53332XG264gbS0NAD69esX83t9++23WblyZeT1rl272LNnT6S/evXqVdmud+/e1K1bl7p169KkSRO2bNnCwoULuemmm2jUqBEADRo0qLSf6lxTkOSi5C5JpW3btsyePfuIul27dvH555/zne98h6KiomPu2DAzLrzwQoqKinjjjTd47LHH6Nq1KzfccAOZmZl88EH5E5KefvrpkXK/fv149NFH2bFjB4WFhVx33XV89dVXnH322ZGz6KMdjztHDh06xJIlS0hNTT1mXXT8lbWrW7dupJySklLpGH1l/ciJ5YQfc5dw6dq1K3v37uXll18GvLPtkSNHMnTo0MgZ8FtvvcWOHTvYt28fr732Gp07d2bTpk2kpaVx++23M2rUKIqKimjTpg3btm2LJPcDBw6wYsWKcvdbv359OnXqxIgRI+jTpw8pKSmceeaZtGrVilmzZgHexd5ly5YB0LlzZ6ZPnw54F0TLc8011/Daa6+xb98+du/ezbx582I+Ht27d+e5556LvK7oP5rqtjvsuuuuY9asWWzfvh0gMiwTaz+SvHTmLpXKy8mr1TBKft/8Y4ZqKmNmzJkzh2HDhvGLX/yCQ4cO0atXL375y19G2lx22WUMHDiQkpISbr/9drKzs3nzzTcZNWoUp5xyCnXq1GHixImcdtppzJ49mwcffJCdO3dSVlbGQw89RGZmZrn7Hjx4MDfddBOLFi2K1E2bNo377ruP8ePHc+DAAW6++Wbat2/Ps88+y6233spTTz1F//79y+2vY8eODB48mPbt29OkSRM6depU7eNw2IQJExg+fDjt2rWjrKyMa665hkmTJtW43WGZmZmMHTuWLl26kJKSwqWXXsrUqVNj7udo+YXf/qxzs3Jje7MSV1U+iel4yM7OdjWdz93Man1BNRmOQbJYtWoVF110UdBhSBKr7HdE87kfX2ZW6JzLLm+dhmVEREJIyV1EJISU3EVEQkjJXUQkhJTcRURCSMldRCSElNwl6aSkpNChQwcuvvhi+vbty3/+858a9bNo0SL69PHmDpo7dy5PPvlkjfqJvsde5ESh5C6Vqu2smzWZhbNevXosXbqU4uJiGjRoUOHEXLHo168fY8aMqdG2Su5yItI3VKVStZ1182ixzsJ55ZVXsnz5cgDWrl3L8OHD2bZtG2lpaUyePJnvfve7DB06lNTUVAoKCti1axfPPPNM5Iz9sKlTp1JQUMDzzz/Pli1buPfee1m3bh0AEydO5Hvf+x4DBgxg48aN7N+/nxEjRpCbm8uYMWPYt28fHTp0IDMzk2nTpvGnP/2JCRMm8M0333D55Zfzu9/9DoC7776bgoICzIy77rqLhx9+OA5HTKRmlNwlaR08eJAFCxZw9913A1Q4/S7A+vXr+eijj1i7di3XXnttZHrg8jz44IN06dKFOXPmcPDgwcjsiVOmTKFBgwbs27ePTp06MXDgQJ588kmef/75yBwrq1atYsaMGbz33nvUqVOHYcOGMW3aNDIzMyktLaW4uBigxkNJJ7o+F2oK7WSh5C5J5/CZcmlpKRdddBHdunWrdPpdgEGDBnHKKafQunVrzj///MhDNcqzcOHCyMRkKSkpnHXWWYA3P8ucOXMA74Egn376aWRq4cMWLFhAYWFhZJ6Yffv20aRJE/r27cu6det44IEH6N27N927d4/PwTjBzLsl9snRJDGU3CXpHB5z37t3Lz169OCFF15g6NChMU2/G+t0vIsWLeLtt9/mgw8+IC0tjZycHPbv339MO+ccd955J7/61a+OWbds2TLefPNNJk2axMyZMyMPBREJgi6oStJKS0tjwoQJPP3006SlpVU4/S7ArFmzOHToEGvXrmXdunW0adOmwn67du3KxIkTAW/oZ+fOnezcuZNzzjmHtLQ0Vq9ezZIlSyLt69Spw4EDByLbzp49m61btwLeVLkbNmzg3//+N4cOHWLgwIGMHz+eoqKiuB8PkVgouUtSu/TSS2nXrh2vvPIK06ZN48UXX6R9+/ZkZmby+uuvR9q1aNGCyy67jOuvv55JkyZV+rCJZ599lnfeeYdLLrmErKwsVq5cSc+ePSkrK+Oiiy5izJgxXHHFFZH2ubm5tGvXjttuu422bdsyfvx4unfvTrt27ejWrRubN2+mtLSUnJwcOnTowO23317umb3I8aQpfzXl7xGOns41PaMFX5RujFv//6tZczaXfB63/sB72HT0M0glsSqb8jdvUd635Vo+TlGqVtmUvxpzl0rFOxFLuEU/b1fJPVhK7nLCmzp1atAhiCQdjbmLiISQkrscQ9cgpCL63ThxKLnLEVJTU9m+fbv+iOUYzjm2b99e6Z1Ikjw05i5HyMjIoKSkhG3btgUdiiSh1NRUMjIygg5DqkHJXY5Qp04dWrVqFXQYIlJLGpYREQmhKpO7mTU3s3fMbKWZrTCzEX59AzN7y8w+9f89x683M5tgZmvMbLmZdUz0mxARkSNV58y9DBjpnGsLXAEMN7O2wBhggXOuNbDAfw1wPdDaX3KBiXGPWkREKlXlmLtzbjOw2S/vNrNVQDOgP5DjN3sJWASM9utfdt7tFkvM7GwzS/f7EZEQu6fjPUGHIL6YLqiaWUvgUuBD4NyohP0FcK5fbgZET0ZS4tcpuYuEXH7f/KBDEF+1L6iaWX3gVeAh59yu6HX+WXpMN0abWa6ZFZhZgW67ExGJr2oldzOrg5fYpznn/uxXbzGzdH99OrDVry8FmkdtnuHXHcE5l++cy3bOZTdu3Lim8YuISDmqc7eMAS8Cq5xzz0Stmgvc6ZfvBF6Pqr/Dv2vmCmCnxttFRI6v6oy5dwaGAP8ys8PPOHsUeBKYaWZ3AxuAQf66N4BewBpgL/DDuEYsIkkrd15upKzx92BV526ZxUBFD6TsWk57BwyvZVwicgKaXDQ5UlZyD5a+oSoiEkJK7iIiIaTkLiISQkruIiIhpOQuIhJCSu4iIiGk5C4iEkJK7iIiIaTkLiISQnqGqojEzeNdHg86BPEpuYtI3OTl5AUdgvg0LCMiEkJK7iIiIaTkLiISQhpzF5G46ftK30h53i3zAoxElNxFJG7mfzI/6BDEp2EZEZEQUnIXEQkhJXcRkRBSchcRCSEldxGREFJyFxEJISV3EZEQUnIXEQkhJXcRkRDSN1RFJG5+3+f3QYcgPiV3EYmb3KzcoEMQn4ZlTkLpGS0wsxot6Rktgg5fRKpBZ+4noS9KN3Le6JpN8LThqT5xjkZEEkFn7iIiIaQzdxGJm6z8rEi5MLcwwEhEyV1E4qZoc1HQIYhPwzIiIiGk5C4iEkJVJnczm2JmW82sOKouz8xKzWypv/SKWveIma0xs4/NrEeiAhcRkYpV58x9KtCznPrfOuc6+MsbAGbWFrgZyPS3+Z2ZpcQrWBERqZ4qk7tz7u/Ajmr21x+Y7pz72jn3GbAGuKwW8YmISA3UZsz9fjNb7g/bnOPXNQM2RrUp8etEROQ4qmlynwhcAHQANgNPx9qBmeWaWYGZFWzbtq2GYYiISHlqlNydc1uccwedc4eAyXw79FIKNI9qmuHXlddHvnMu2zmX3bhx45qEISIiFahRcjez9KiXNwCH76SZC9xsZnXNrBXQGviodiGKiEisqvyGqpm9AuQAjcysBHgcyDGzDoAD1gM/BnDOrTCzmcBKoAwY7pw7mJjQRSTZzL15btAhiK/K5O6cu6Wc6hcraf8E8ERtghKRE1PfNn2DDkF8+oaqiEgIKbmLiISQkruISAhpyl8RiZumTzeNlDeN3BRgJKLkLiJxs3nP5qBDEJ+GZUREQkjJXUQkhJTcRURCSMldRCSElNxFREJIyV1EJISU3EVEQkjJXUQkhJTcRURCSN9QFZG4KbinIOgQxKfkLiJxk9U0K+gQxKdhGRGREFJyFxEJISV3EZEQ0pi7iMSNjbNI2T3uAoxEdOYuIhJCSu4iIiGk5C4iEkJK7iIiIaTkLiISQkruIiIhpOQuIhJCSu4iIiGk5C4iEkJK7iIiIaTpB0Qkbkr/d2nQIYhPyV1E4qbpGU2DDkF8GpYREQkhJXcRkRDSsIyIxM2m3ZsiZQ3RBKvKM3czm2JmW82sOKqugZm9ZWaf+v+e49ebmU0wszVmttzMOiYyeBFJLs2eaRZZJFjVGZaZCvQ8qm4MsMA51xpY4L8GuB5o7S+5wMT4hCkiIrGoMrk75/4O7Diquj/wkl9+CRgQVf+y8ywBzjaz9HgFKyIi1VPTC6rnOuc2++UvgHP9cjNgY1S7Er9ORESOo1rfLeOcc0DMD0s0s1wzKzCzgm3bttU2DBERiVLT5L7l8HCL/+9Wv74UaB7VLsOvO4ZzLt85l+2cy27cuHENwxARkfLUNLnPBe70y3cCr0fV3+HfNXMFsDNq+EZERI6TKu9zN7NXgBygkZmVAI8DTwIzzexuYAMwyG/+BtALWAPsBX6YgJhFRKQKVSZ359wtFazqWk5bBwyvbVAiIlI7mn5ARCSElNxFREJIc8uISNy4x2O+K1oSRGfuIiIhpOQuIhJCSu4iIiGkMXcRiZvCTYWRclbTrAAjESV3EYmb7MnZkbIurgZLwzIiIiGk5C4iEkJK7iIiIaTkLiISQkruIiIhpOQuIhJCSu4iIiGk5C4iEkJK7iIiIaRvqIpI3KTXTw86BPEpuYtI3GwauSnoEMSnYRkRkRBSchcRCSEldxGRENKYu4jEzbyP50XKfdv0DTASUXIXkbjpN71fpKz53IOlYRkRkRBSchcRCSEldxGREFJyFxEJISV3EZEQUnIXqUJ6RgvMrEZLekaLoMOXk5RuhRSpwhelGzlv9PwabbvhqT5xjkakenTmLiISQkruIiIhpGEZEYmbjukdgw5BfLVK7ma2HtgNHATKnHPZZtYAmAG0BNYDg5xzX9YuTBE5ERTmFgYdgvjiMSxzrXOug3Mu2389BljgnGsNLPBfi4jIcZSIMff+wEt++SVgQAL2ISIilahtcnfAX82s0Mxy/bpznXOb/fIXwLm13IeIiMSothdUr3LOlZpZE+AtM1sdvdI558ys3Hk//f8McgFatNAXPUTCIL8wP1LOzcqtpKUkWq2Su3Ou1P93q5nNAS4DtphZunNus5mlA1sr2DYfyAfIzs7WxM8iIfDj+T+OlJXcg1XjYRkzO93MzjhcBroDxcBc4E6/2Z3A67UNUkREYlObM/dzgTlmdrif/3LO/X8z+wcw08zuBjYAg2ofpoiIxKLGyd05tw5oX079dqBrbYISEZHa0fQDIiIhpOQuIhJCSu4iIiGk5C4iEkJK7iIiIaTkLiISQprPXUTips+FeqxgslByF5G4mXfLvKBDEJ+GZUREQkjJXUQkhJTcRURCSMldROImb1Ee9fudhV1rmMW+pGfo2Q7xoguqIhI34/42DrK88nmXz495+w1P6W6beNGZu4hICCm5i4iEkJK7iEgIKbmLiISQkruISAgpuYuIhJCSu4hICCm5i4iEkJK7iEgIKbmLSNzc0/EeKIT6ZT2CDuWkp+QuInGT3zcf5kHDAw8EHcpJT8ldRCSElNxFREJIyV1EJISU3EUkbnLn5UJf2F7nuaBDOekpuYtI3EwumgxZsOfUN4MO5aSn5C4iEkJK7iIiIaTkLiISQkruIiIhpOQuIlJL6RktMLMaLekZLRIS06kJ6RUws57As0AK8Afn3JOJ2peISJC+KN3IeaPn12jbDU/1iXM0noScuZtZCvACcD3QFrjFzNomYl8iInKsRA3LXAascc6tc859A0wH+idoXyIicpREJfdmwMao1yV+nYiIHAfmnIt/p2Y/AHo6537kvx4CXO6cuz+qTS6Q679sA3xcw901Av5di3ATJVnjguSNTXHFRnHFJoxxneeca1zeikRdUC0Fmke9zvDrIpxz+UB+bXdkZgXOueza9hNvyRoXJG9siis2iis2J1tciRqW+QfQ2sxamdlpwM3A3ATtS0REjpKQM3fnXJmZ3Q+8iXcr5BTn3IpE7EtERI6VsPvcnXNvAG8kqv8otR7aSZBkjQuSNzbFFRvFFZuTKq6EXFAVEZFgafoBEZEQSrrkbmY9zexjM1tjZmPKWV/XzGb46z80s5ZR6x7x6z82sx5R9evN7F9mttTMCo5nXGbW0MzeMbM9Zvb8Udtk+XGtMbMJZmZJEtciv8+l/tLkOMbVzcwK/eNSaGbXRW0T5PGqLK4gj9dlUftdZmY3VLfPAOMK7O8xan0L/3f/J9XtM8C4ana8nHNJs+BdfF0LnA+cBiwD2h7VZhgwyS/fDMzwy2399nWBVn4/Kf669UCjgOI6HbgKuBd4/qhtPgKuAAz4b+D6JIlrEZAd0PG6FGjqly8GSpPkeFUWV5DHKw041S+nA1vxrqVV2WcQcQX99xi1fjYwC/hJdfsMIq7aHK9kO3OvzrQF/YGX/PJsoKt/BtcfmO6c+9o59xmwxu8v0Licc1855xYD+6Mbm1k6cKZzbonzfoIvAwOCjitOahPXP51zm/z6FUA9/2wn6ONVblwx7j8Rce11zpX59anA4Yto8ZgCJBFxxUNt8gRmNgD4DO/nGEufQcRVY8mW3KszbUGkjf/LsxNoWMW2Dvir/3E6l9jVJq7K+iypos8g4jrsj/7HwJ/WYPgjXnENBIqcc1+TXMcrOq7DAjteZna5ma0A/gXc66+PxxQgiYgLAvx7NLP6wGhgXA36DCIuqOHxStitkEnmKudcqT8W+paZrXbO/T3ooJLYbf7xOgN4FRiCd6Z83JhZJvAU0P147rcqFcQV6PFyzn0IZJrZRcBLZvbfx2vflSkvLufcfoL9e8wDfuuc2xP7/8EJlUfFcdXoeCXbmXuV0xZEtzGzU4GzgO2VbeucO/zvVmAOsQ/X1CauyvrMqKLPIOKKPl67gf/iOB8vM8vA+znd4ZxbG9U+0ONVQVyBH6+oOFYBe/CvCVSjzyDiCvrv8XLg/5jZeuAh4FHzvnAZ9PGqKK6aH69YB+kTueB9kliHd0H08AWJzKPaDOfICxIz/XImR15QXYd3geN04Ay/zenA+3iTmh2XuKLWD6XqC6q9go7L77ORX66DNy5473H8OZ7tt7+xnH4DO14VxZUEx6sV316oPA/YhDcRVZV9BhRXUvw9+vV5fHtBNdDjVUlcNT5e1Q78eC1AL+ATvKvOY/26nwP9/HIq3tXkNXh/7OdHbTvW3+5j/Dsp8K5cL/OXFYf7PM5xrQd24J29lOBfQQeygWK/z+fxv1QWZFz+L1AhsNw/Xs/i33V0POICHgO+ApZGLU2CPl4VxZUEx2uIv9+lQBEwoLI+g46LJPh7jOojjyPvSgnseFUUV22Ol76hKiISQsk25i4iInGg5C4iEkJK7iIiIaTkLiISQkruIiIhpOQutWZmB/2v3q/wZwAcaWan+OuyzWyCX65rZm/7bQeb2dX+NkvNrF6w76J8ZvaGmZ0ddBwisdKtkFJrZrbHOVffLzfB+5bme865x49qdwUw3jn3ff/1JGCxc+5P1dyP4f3OHorrGzgB6VhIVXTmLnHlvK9I5wL3myfHzOb7Sf9PQCf/TP3HwCDgF2Y2DcDMRpnZP8xsuZmN8+tamjc/9st4X2BqXkm7VWY22f808NfDnwbM7Dv+J4ZlZlZkZhdUtL+jmTeXdqPK+j+qfV/z5un+p7/Pc/36+mb2R/Pm5V5uZgP9+p5+TMvMbIFfl2dHzudd7O+/vGMx0cwK/JjGRW3Tycze9/v9yMzOMLO/m1mHqDaLzax9zX7SkvRq8i0sLVqiF2BPOXX/Ac4FcoD5fl2k7L+eCvzAL3fHe5ak4Z10zAeuAVoCh4ArqtGuDOjgt5sJ3O6XPwRu8MupeHONl9tPOe9jPd7X5ivs/6j25/DtJ+IfAU/75aeA/3tUu8Z4MwS28usa+P/mceQ3J4v9/R9xLI7aJgVvXvl2eF99Xwd08tediffV+DsPxwBcCBQE/bujJXHLyTIrpCS/7v7yT/91faA18DmwwTm3pBrtPnPOLfXrC4GW5s3U2Mw5NwfAebMSYmYV9VPZbHvH9F9Omwxghnnzz5+GNz83wPfx5hLBj+NLM+sL/N15zx/AObejkn0fFn0sAAaZNw3sqXgPxWiLN0XsZufcP/x+d/nveRbwUzMbBdyF95+rhJSSu8SdmZ0PHMR7+s5F1d0M+JVz7vdH9dUSb06X6rSLnl/9IFDZRdpy+6lCdfp/DnjGOTfXzHLwzsJjVcaRQ6apUeXIsTCzVsBP8M7QvzSzqUe1PYJzbq+ZvYX3wIhBQFYNYpMThMbcJa7MrDEwCW+myViu1r8J3GXeQwsws2ZW/rNIq9sOiEzDW2LeU24O37GTFms/MTiLb6d5vTOq/i28GQHx93cOsAS4xk/SmFkDf/V6oKNf1xFvlsHynImX7Hf6Y/vX+/UfA+lm1snv4wzzppcF+AMwAfiHc+7LGr5HOQHozF3ioZ6ZLcWb8rYM+H/AM7F04Jz7q3kPdfjAuxGEPcDteGfIMbc7yhDg92b2c+AAcFMl/WyNJe5y5AGzzOxLYCHfJubxwAtmVuzHOs4592d/SOXP5t06uhXohvfAjzvMe4rRh3izDB7DObfMzP4JrMYbu3/Pr//GzAYDz/kXfffhDQvtcc4Vmtku4I+1fJ+S5HQrpMhJxMya4l14/a7TbZShpmEZkZOEmd2B90lgrBJ7+OnMXUQkhHTmLiISQkruIiIhpOQuIhJCSu4iIiGk5C4iEkJK7iIiIfQ/WPajwUJzKXsAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}